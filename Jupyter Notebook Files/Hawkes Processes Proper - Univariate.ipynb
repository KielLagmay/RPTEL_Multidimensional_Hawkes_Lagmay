{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import threading\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ast import literal_eval\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pytz\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from tick.plot import plot_hawkes_kernels, plot_point_process\n",
    "from tick.hawkes import HawkesExpKern, HawkesADM4, SimuHawkesExpKernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(str(input()))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.drop(['D'], axis=1, inplace = True)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32852df",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampColumns = ['A', 'C', 'L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5503c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = datetime.strptime(str(input(\"Start Date (Format: YYYY-MM-DD): \")), \"%Y-%m-%d\").astimezone(pytz.utc)\n",
    "startDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "endDate = datetime.strptime(str(input(\"End Date (Format: YYYY-MM-DD): \")), \"%Y-%m-%d\").astimezone(pytz.utc)\n",
    "endDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c62800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dayAfterEndDate = float((time.mktime(endDate.timetuple()) / 3600) - (time.mktime(startDate.timetuple()) / 3600))\n",
    "dayAfterEndDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de60dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateActualIntensities(timestampList):\n",
    "    result = {}\n",
    "    for timePoint in timestampList:\n",
    "        if math.floor(timePoint) not in result.keys():\n",
    "            result[math.floor(timePoint)] = 0\n",
    "        result[math.floor(timePoint)] = result[math.floor(timePoint)] + 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "for column in timestampColumns:\n",
    "    dataset[column] = dataset[column].apply(lambda x: literal_eval(x))\n",
    "    dataset[column] = dataset[column].apply(sorted)\n",
    "    dataset['actual_intensities_' + column] = dataset[column].apply(calculateActualIntensities)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineColumns(df, columns):\n",
    "    return [np.array(df[column], dtype=np.double) for column in columns]\n",
    "\n",
    "def combineDimensions(df, timestampDimensions):\n",
    "    result = []\n",
    "    for timestampDimension in timestampDimensions:\n",
    "        result = result + df[timestampDimension]\n",
    "    \n",
    "    return np.sort(np.array(result, dtype=np.double))\n",
    "\n",
    "dataset['timestamps'] = dataset.apply(lambda df: combineColumns(df, timestampColumns), axis = 1)\n",
    "dataset['timestamps_train'] = dataset.apply(lambda df: [combineDimensions(df, timestampColumns)], axis = 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateBestDecay(timestamps):\n",
    "    highestScore = None\n",
    "    bestDecay = None\n",
    "    for d in range(1, 11):\n",
    "        learner = HawkesExpKern(decays=d, penalty='elasticnet', elastic_net_ratio=0.5)\n",
    "        learner.fit(timestamps)\n",
    "        score = learner.score(timestamps, end_times=dayAfterEndDate)\n",
    "        if((highestScore == None) or (score >= highestScore)):\n",
    "            highestScore = score\n",
    "            bestDecay = d\n",
    "    \n",
    "    return bestDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b82902",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    decays = pd.read_csv(str(input()))\n",
    "    dataset = dataset.merge(decays, on=['metadata_context_id', 'metadata_user_id'])\n",
    "except:\n",
    "    dataset['decay'] = dataset['timestamps_train'].apply(estimateBestDecay)\n",
    "    dataset[['metadata_context_id', 'metadata_user_id', 'decay']].to_csv('UnivariateHawkesDecays_Intercession_2022.csv', header = True, index = False)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHawkesProcessExpKernAdjacency(df):\n",
    "    learner = HawkesExpKern(decays=df['decay'], penalty='elasticnet', elastic_net_ratio=0.5)\n",
    "    learner.fit(df['timestamps_train'])\n",
    "    return learner.adjacency\n",
    "\n",
    "dataset['adjacency'] = dataset.apply(getHawkesProcessExpKernAdjacency, axis = 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHawkesProcessExpKernBaseline(df):\n",
    "    learner = HawkesExpKern(decays=df['decay'], penalty='elasticnet', elastic_net_ratio=0.5)\n",
    "    learner.fit(df['timestamps_train'])\n",
    "    return learner.baseline\n",
    "\n",
    "dataset['baseline'] = dataset.apply(getHawkesProcessExpKernBaseline, axis = 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHawkesProcessExpKernIntensities(df, dim):\n",
    "    learner = HawkesExpKern(decays=df['decay'], penalty='elasticnet', elastic_net_ratio=0.5)\n",
    "    learner.fit(df['timestamps_train'])\n",
    "    intensities, intensity_times = learner.estimated_intensity([df['timestamps'][dim]], intensity_track_step=1, end_time=dayAfterEndDate)\n",
    "    \n",
    "    return intensities\n",
    "\n",
    "for i in range(0, len(timestampColumns)):\n",
    "    dataset['intensities_' + timestampColumns[i]] = dataset.apply(lambda df: getHawkesProcessExpKernIntensities(df, i), axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685177bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getHawkesProcessExpKernIntensityTimes(df, dim):\n",
    "    learner = HawkesExpKern(decays=df['decay'], penalty='elasticnet', elastic_net_ratio=0.5)\n",
    "    learner.fit(df['timestamps_train'])\n",
    "    intensities, intensity_times = learner.estimated_intensity([df['timestamps'][dim]], intensity_track_step=1, end_time=dayAfterEndDate)\n",
    "    \n",
    "    return intensity_times\n",
    "\n",
    "for i in range(0, len(timestampColumns)):\n",
    "    dataset['intensity_times_' + timestampColumns[i]] = dataset.apply(lambda df: getHawkesProcessExpKernIntensityTimes(df, i), axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ce54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitResultsExpKern(overallResult, dimensions):\n",
    "    for i in range(0, len(dimensions)):\n",
    "        overallResult['baseline_' + dimensions[i]] = overallResult['baseline'].apply(lambda b: b.tolist()[i])\n",
    "        \n",
    "        for j in range(0, len(dimensions)):\n",
    "            overallResult['influence_' + dimensions[i] + dimensions[j]] = overallResult['adjacency'].apply(lambda a: a.tolist()[i][j])\n",
    "    \n",
    "    overallResult.drop(['baseline', 'adjacency'], axis = 1, inplace = True)\n",
    "    return overallResult\n",
    "\n",
    "dataset = splitResultsExpKern(dataset, ['U'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestampColumn in timestampColumns:\n",
    "    dataset['intensities_' + timestampColumn] = dataset['intensities_' + timestampColumn].apply(lambda x: x[0].tolist())\n",
    "    dataset['intensity_times_' + timestampColumn] = dataset['intensity_times_' + timestampColumn].apply(lambda x: x.tolist())\n",
    "    dataset['intensities_' + timestampColumn] = dataset.apply(lambda df: dict(zip(df['intensity_times_' + timestampColumn], df['intensities_' + timestampColumn])), axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixTimestamps(df, column):\n",
    "    result = {}\n",
    "    for h in df[column].keys():\n",
    "        if math.floor(h) not in result.keys():\n",
    "            result[math.floor(h)] = 0\n",
    "        result[math.floor(h)] = result[math.floor(h)] + df[column][h]\n",
    "    return result\n",
    "\n",
    "for dim in timestampColumns:\n",
    "    dataset['intensities_' + dim] = dataset.apply(lambda df: fixTimestamps(df, 'intensities_' + dim), axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeTimestamps(df, colName):\n",
    "    result = {}\n",
    "    for t in range(0, math.floor(dayAfterEndDate) + 1):\n",
    "        if t not in df[colName].keys():\n",
    "            result[t] = 0\n",
    "        else:\n",
    "            result[t] = df[colName][t]\n",
    "    return result\n",
    "\n",
    "for colName in ['actual_intensities_', 'intensities_']:\n",
    "    for dim in timestampColumns:\n",
    "        dataset[colName + dim] = dataset.apply(lambda df: completeTimestamps(df, colName + dim), axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926423bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = dataset['Modality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineIntensityDictionaries(x, y):\n",
    "    combinedDictionary = {}\n",
    "    \n",
    "    for key in x.keys():\n",
    "        if key not in combinedDictionary.keys():\n",
    "            combinedDictionary[key] = 0\n",
    "        combinedDictionary[key] = combinedDictionary[key] + x[key]\n",
    "    for key in y.keys():\n",
    "        if key not in combinedDictionary.keys():\n",
    "            combinedDictionary[key] = 0\n",
    "        combinedDictionary[key] = combinedDictionary[key] + y[key]\n",
    "    \n",
    "    return combinedDictionary\n",
    "    \n",
    "\n",
    "def sumIntensities(series):\n",
    "    return reduce(combineIntensityDictionaries, series)\n",
    "\n",
    "intensityColumns = {}\n",
    "for dimension in timestampColumns:\n",
    "    intensityColumns['actual_intensities_' + dimension] = sumIntensities\n",
    "    intensityColumns['intensities_' + dimension] = sumIntensities\n",
    "\n",
    "intensityPlotDataset = dataset.groupby(['Modality']).agg(intensityColumns).reset_index()\n",
    "intensityPlotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "courseStudentCount = dataset.groupby(['Modality'])['course_code'].count().to_frame('total').reset_index()\n",
    "courseStudentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c664773",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensityPlotDataset = intensityPlotDataset.merge(courseStudentCount, on=['Modality'])\n",
    "intensityPlotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43694e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageIntensities(df, column):\n",
    "    result = {}\n",
    "    for k in sorted([key for key in df[column].keys()]):\n",
    "        result[k] = df[column][k] / df['total']\n",
    "    return result\n",
    "\n",
    "for column in ['actual_intensities_' + dimension for dimension in timestampColumns] + ['intensities_' + dimension for dimension in timestampColumns]:\n",
    "    intensityPlotDataset[column] = intensityPlotDataset.apply(lambda df: getAverageIntensities(df, column), axis = 1)\n",
    "\n",
    "intensityPlotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modality in modalities:\n",
    "    fig, axs = plt.subplots(len(timestampColumns), 1)\n",
    "    intensityPlotDatasetModality = intensityPlotDataset.loc[intensityPlotDataset['Modality'] == modality]\n",
    "    \n",
    "    for i in range(0, len(timestampColumns)):\n",
    "        axs[i].plot([t for t in intensityPlotDatasetModality.iloc[0]['actual_intensities_' + timestampColumns[i]].keys()], [v for v in intensityPlotDatasetModality.iloc[0]['actual_intensities_' + timestampColumns[i]].values()], color='tab:blue')\n",
    "        axs[i].plot([t for t in intensityPlotDatasetModality.iloc[0]['intensities_' + timestampColumns[i]].keys()], [v for v in intensityPlotDatasetModality.iloc[0]['intensities_' + timestampColumns[i]].values()], color='tab:orange')\n",
    "        axs[i].set_title(timestampColumns[i])\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Time', ylabel='Intensity')\n",
    "    \n",
    "    fig.legend(handles=[Line2D([0], [0], color='tab:blue', label='Actual Intensities'), Line2D([0], [0], color='tab:orange', label='Est. Intensities')], loc='lower center')\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(10)\n",
    "    fig.savefig(modality + '_Intensities_Univariate.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in timestampColumns:\n",
    "    dataset['rmse_' + dim] = dataset.apply(lambda df: mean_squared_error([v for v in df['actual_intensities_' + dim].values()], [v for v in df['intensities_' + dim].values()], squared=False), axis = 1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "averageAgg = {}\n",
    "for dim in timestampColumns:\n",
    "    averageAgg['rmse_' + dim] = np.mean\n",
    "\n",
    "rmseModalities = dataset.groupby(['Modality']).agg(averageAgg).reset_index()\n",
    "rmseModalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseModalities.to_csv('RMSE_Univariate.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a146332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
